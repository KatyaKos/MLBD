{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Movie-recommendation\" data-toc-modified-id=\"Movie-recommendation-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Movie recommendation</a></span><ul class=\"toc-item\"><li><span><a href=\"#Dataset\" data-toc-modified-id=\"Dataset-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Dataset</a></span></li><li><span><a href=\"#Evaluation-Protocol\" data-toc-modified-id=\"Evaluation-Protocol-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Evaluation Protocol</a></span></li><li><span><a href=\"#Models\" data-toc-modified-id=\"Models-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Models</a></span><ul class=\"toc-item\"><li><span><a href=\"#ALS\" data-toc-modified-id=\"ALS-1.3.1\"><span class=\"toc-item-num\">1.3.1&nbsp;&nbsp;</span><a href=\"https://spark.apache.org/docs/latest/ml-collaborative-filtering.html#explicit-vs-implicit-feedback\" target=\"_blank\">ALS</a></a></span></li><li><span><a href=\"#Ваша-формулировка\" data-toc-modified-id=\"Ваша-формулировка-1.3.2\"><span class=\"toc-item-num\">1.3.2&nbsp;&nbsp;</span>Ваша формулировка</a></span></li></ul></li><li><span><a href=\"#Evaluation-Results\" data-toc-modified-id=\"Evaluation-Results-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Evaluation Results</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movie recommendation\n",
    "\n",
    "Ваша задача - рекомендация фильмов для пользователей\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format ='retina'\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import pyspark\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master('local[*]') \\\n",
    "    .appName(\"spark_sql_examples\") \\\n",
    "    .config(\"spark.executor.memory\", \"40g\") \\\n",
    "    .config(\"spark.driver.memory\", \"40g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset \n",
    "\n",
    "`MovieLens-25M`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "genome-scores.csv  links.csv   rating_graph.edgelist  README.txt\r\n",
      "genome-tags.csv    movies.csv  ratings.csv\t      tags.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls /workspace/ml_bd/ml-25m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '/workspace/ml_bd/ml-25m'\n",
    "\n",
    "RATINGS_PATH = os.path.join(DATA_PATH, 'ratings.csv')\n",
    "MOVIES_PATH = os.path.join(DATA_PATH, 'movies.csv')\n",
    "TAGS_PATH = os.path.join(DATA_PATH, 'tags.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "\n",
    "ratings_df = sqlContext.read.format(\"com.databricks.spark.csv\") \\\n",
    "    .option(\"delimiter\", \",\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load('file:///' + RATINGS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df = sqlContext.read.format(\"com.databricks.spark.csv\") \\\n",
    "    .option(\"delimiter\", \",\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load('file:///' + MOVIES_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(userId=1, movieId=296, rating=5.0, timestamp=1147880044)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_df.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(movieId=1, title='Toy Story (1995)', genres='Adventure|Animation|Children|Comedy|Fantasy')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_df.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Protocol\n",
    "\n",
    "Так как мы хотим оценивать качество разных алгоритмов рекомендаций, то в первую очередь нам нужно определить\n",
    "* Как разбить данные на `Train`/`Validation`/`Test`;\n",
    "* Какие метрики использовать для оценки качества."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metrics import get_ate\n",
    "from processing import split_by_col, partitioned_split_by_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Protocol 1 --- split by timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = split_by_col(ratings_df, 'timestamp', [0.85, 0.15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of users in test but not in train: 17993\n"
     ]
    }
   ],
   "source": [
    "new_test_users_num = test_df.select('userId').distinct().subtract(train_df.select('userId').distinct()).count()\n",
    "print(\"Number of users in test but not in train:\", new_test_users_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of users in train: 144548\n",
      "Number of users in test: 24525\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of users in train:\", train_df.select('userId').distinct().count())\n",
    "print(\"Number of users in test:\", test_df.select('userId').distinct().count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number in train: 21250080\n",
      "Number in test: 3750014\n"
     ]
    }
   ],
   "source": [
    "print(\"Number in train:\", train_df.count())\n",
    "print(\"Number in test:\", test_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_users = test_df.select('userId').distinct().intersect(train_df.select('userId').distinct()).collect()\n",
    "good_users = [user['userId'] for user in good_users]\n",
    "test_df = test_df\\\n",
    "    .filter(F.col('userId').isin(good_users))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Protocol 2 --- split each user by timestamp and unite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df, test_df = partitioned_split_by_cols(ratings_df, 'userId', 'timestamp', [0.8, 0.1, 0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of users in val but not in train: 0\n",
      "Number of users in test but not in train: 0\n"
     ]
    }
   ],
   "source": [
    "new_test_users_num = test_df.select('userId').distinct().subtract(train_df.select('userId').distinct()).count()\n",
    "new_val_users_num = val_df.select('userId').distinct().subtract(train_df.select('userId').distinct()).count()\n",
    "print(\"Number of users in val but not in train:\", new_val_users_num)\n",
    "print(\"Number of users in test but not in train:\", new_test_users_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of users in train: 162541\n",
      "Number of users in val: 158503\n",
      "Number of users in test: 157054\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of users in train:\", train_df.select('userId').distinct().count())\n",
    "print(\"Number of users in val:\", val_df.select('userId').distinct().count())\n",
    "print(\"Number of users in test:\", test_df.select('userId').distinct().count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number in train: 20017461\n",
      "Number in val: 2477991\n",
      "Number in test: 2367777\n"
     ]
    }
   ],
   "source": [
    "print(\"Number in train:\", train_df.count())\n",
    "print(\"Number in val:\", val_df.count())\n",
    "print(\"Number in test:\", test_df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Protocols consensus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В первом способе получилось слишком много новых пользователей в тесте и валидейте, так что использовать в так виде или оставлять только \"общих\" пользователей кажется бессмысленным. Во втором способе этой проблемы нет. Теоретическая проблема: заглядывание в будущее. Но мы по сути заглядываем в будущее для оценок фильмов. А фильмы это фиксированный объект, со временем не меняется. Так что кажется, что ничего страшного в этом нет.\n",
    "\n",
    "Далее используется разделение датасета вторым способом.\n",
    "\n",
    "Считаем, что стоит рекомендовать посмотреть, если рейтинг поставленный >=3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_df.filter(F.col('rating') >= 3.)\n",
    "val_df = val_df.filter(F.col('rating') >= 3.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Protocols consensus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При рекомендации фильмов обычно число k в top-k не очень большое, так что, кажется, порядок не так важен. Интереса ради используется одна метрика порядка (MAP). Но основными метриками будут Precision@k, Recall@k; [1, 5, 10]. HR@k не используется, так как не очень интересно, сколько раз удалось пользователю посоветовать ХОТЬ ОДИН хороший фильм. Хочется скорее узнать среднее число удачных советов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_at_k(recommended_df, true_df, k=10):\n",
    "    def limiter(lst):\n",
    "        return lst[:k]\n",
    "\n",
    "    limiter_udf = F.udf(limiter, ArrayType(IntegerType()))\n",
    "\n",
    "    users_num = true_df.select('userId').distinct().count()\n",
    "    result = recommended_df\\\n",
    "        .withColumn('recommendations', limiter_udf('recommendations'))\\\n",
    "        .join(true_df, 'userId')\\\n",
    "        .withColumn('common', F.size(F.array_intersect('recommendations', 'golds')) / F.size(F.col('golds')))\\\n",
    "        .select('common')\\\n",
    "        .groupBy().agg(F.sum('common').alias('sum')).take(1)[0]['sum']\n",
    "    return result / users_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "from pyspark.mllib.evaluation import RankingMetrics\n",
    "\n",
    "def evaluate_movies(recommend_df, gold_df, tops=[1, 5, 10]):\n",
    "    metrics = {}\n",
    "    golds = gold_df\\\n",
    "        .groupBy('userId')\\\n",
    "        .agg(F.collect_set('movieId').alias('golds'))\n",
    "    rdd = golds\\\n",
    "        .join(recommend_df, 'userId') \\\n",
    "        .select('recommendations', 'golds') \\\n",
    "        .rdd\n",
    "    ranking_metrics = RankingMetrics(rdd)\n",
    "    \n",
    "    for i in tops:\n",
    "        metrics['precision@' + str(i)] = ranking_metrics.precisionAt(i)\n",
    "        metrics['recall@' + str(i)] = recall_at_k(recommend_df, golds, i)\n",
    "    metrics['MAP'] = ranking_metrics.meanAveragePrecision\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n",
    "\n",
    "Теперь мы можем перейти к формулировке задачи в терминах машинного обучения.\n",
    "\n",
    "Одна из формулировок, к которой мы сведем нашу задачу - **Matrix Completetion**. Данную задачу будем решать с помощью `ALS`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [ALS](https://spark.apache.org/docs/latest/ml-collaborative-filtering.html#explicit-vs-implicit-feedback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_als_space = {\n",
    "    'rank': 10,\n",
    "    'maxIter': 10, \n",
    "    'regParam': 0.1, \n",
    "    'numUserBlocks': 10, \n",
    "    'numItemBlocks': 10, \n",
    "    'implicitPrefs': False, \n",
    "    'alpha': 1.0, \n",
    "    'userCol': 'userId', \n",
    "    'itemCol': 'movieId', \n",
    "    'seed': 42, \n",
    "    'ratingCol': 'rating', \n",
    "    'nonnegative': False, \n",
    "    'checkpointInterval': 10, \n",
    "    'intermediateStorageLevel': 'MEMORY_AND_DISK', \n",
    "    'finalStorageLevel': 'MEMORY_AND_DISK', \n",
    "    'coldStartStrategy': 'nan'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.recommendation import ALS\n",
    "\n",
    "def train_als(space, df):\n",
    "    model = ALS(**space)\n",
    "    model = model.fit(df)\n",
    "    watched = train_df\\\n",
    "        .groupby('userId')\\\n",
    "        .agg(F.collect_set('movieId').alias('watched'))\\\n",
    "        .withColumnRenamed('userId', 'user')\n",
    "    return model, watched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(user=148, watched=[3681, 1199, 589, 3000, 92259, 1222, 1193, 1172, 1266, 44191, 912, 134853, 527, 19, 2959, 608, 104879, 77455, 4993, 2176, 50, 68954, 296, 1089, 1233, 81834, 908, 5060, 858, 116897, 2324, 1206, 57669, 112556, 48516, 750, 1208, 111, 109487, 904, 1252, 99114, 7132, 2951, 919, 78499, 6016, 1196, 115617, 79132, 1198, 923, 1250, 54286, 1221, 122886, 32, 7153, 1244, 1267, 1136, 48394, 1217, 1086, 2502, 2858, 117533, 106100, 2329, 58559, 1292, 5952, 88810, 152059, 6787, 3462, 3949, 1213, 4886, 909, 2019, 134130, 1207, 953, 924, 8228, 1209, 955, 541, 1080, 2067, 1203, 318, 593, 1276, 899, 27773, 60069, 260, 1197, 63082, 922, 1270, 2571, 110])]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.recommendation import ALS\n",
    "base_als = ALS(**baseline_als_space)\n",
    "base_als_model, watched_df = train_als(baseline_als_space, train_df)\n",
    "watched_df = watched_df.cache()\n",
    "watched_df.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Покажите для выбранных вами фильмов топ-20 наиболее похожих фильмов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=10, features=[-0.3731934428215027, 0.6107021570205688, 0.31835755705833435, 0.347337543964386, 0.7369728088378906, -0.7249520421028137, -0.4799730181694031, 0.1913396418094635, 0.8426313996315002, -0.6113075017929077]),\n",
       " Row(id=20, features=[-0.46242234110832214, 0.47487494349479675, -0.004481089301407337, 0.02912776544690132, 0.4901317358016968, -0.896612286567688, -0.26179176568984985, 0.3643686771392822, 0.8532597422599792, -0.1589564085006714])]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_als_vectors = base_als_model.itemFactors\n",
    "base_als_vectors.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommendations_to_titles(data):\n",
    "    tmp = movies_df.select('movieId', 'title')\\\n",
    "        .join(data, movies_df['movieId'] == data['baseId'])\\\n",
    "        .drop('movieId')\\\n",
    "        .withColumnRenamed('title', 'baseTitle')\n",
    "    return movies_df.select('movieId', 'title')\\\n",
    "        .join(tmp, movies_df['movieId'] == tmp['recommendationId'])\\\n",
    "        .drop('movieId')\\\n",
    "        .withColumnRenamed('title', 'recommendationTitle')\\\n",
    "        .sort(F.col('score').desc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf, array\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "\n",
    "def cosine_sim(array):\n",
    "        return float(1 - cosine(array[0], array[1]))\n",
    "\n",
    "\n",
    "def find_similar_movie(vectors_df, movieIds, topK):\n",
    "    base_df = vectors_df\\\n",
    "        .filter(F.col('id').isin(movieIds))\\\n",
    "        .withColumnRenamed('id', 'baseId')\\\n",
    "        .withColumnRenamed('features', 'baseFeatures')\n",
    "    \n",
    "    dot_udf = udf(cosine_sim, FloatType())\n",
    "\n",
    "    \n",
    "    result = vectors_df\\\n",
    "        .join(base_df, base_df['baseId'] != vectors_df['id'])\\\n",
    "        .withColumn('score', dot_udf(array('features', 'baseFeatures')))\\\n",
    "        .sort(F.col('score').desc())\\\n",
    "        .limit(topK)\\\n",
    "        .drop('features', 'baseFeatures')\\\n",
    "        .withColumnRenamed('id', 'recommendationId')\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(movieId=260, title='Star Wars: Episode IV - A New Hope (1977)', genres='Action|Adventure|Sci-Fi')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_df.filter(F.col('movieId')==260).take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(recommendationTitle='Star Wars: Episode V - The Empire Strikes Back (1980)', baseTitle='Star Wars: Episode IV - A New Hope (1977)', recommendationId=1196, baseId=260, score=0.9970183372497559),\n",
       " Row(recommendationTitle='Star Wars: Episode VI - Return of the Jedi (1983)', baseTitle='Star Wars: Episode IV - A New Hope (1977)', recommendationId=1210, baseId=260, score=0.9899550676345825),\n",
       " Row(recommendationTitle='Floating Skyscrapers (2014)', baseTitle='Star Wars: Episode IV - A New Hope (1977)', recommendationId=129841, baseId=260, score=0.9857679009437561),\n",
       " Row(recommendationTitle='Raiders of the Lost Ark (Indiana Jones and the Raiders of the Lost Ark) (1981)', baseTitle='Star Wars: Episode IV - A New Hope (1977)', recommendationId=1198, baseId=260, score=0.9824265241622925),\n",
       " Row(recommendationTitle='Star Trek II: The Wrath of Khan (1982)', baseTitle='Star Wars: Episode IV - A New Hope (1977)', recommendationId=1374, baseId=260, score=0.9775526523590088),\n",
       " Row(recommendationTitle='Indiana Jones and the Last Crusade (1989)', baseTitle='Star Wars: Episode IV - A New Hope (1977)', recommendationId=1291, baseId=260, score=0.9744508266448975),\n",
       " Row(recommendationTitle=\"The League of Gentlemen's Apocalypse (2005)\", baseTitle='Star Wars: Episode IV - A New Hope (1977)', recommendationId=162422, baseId=260, score=0.9742730259895325),\n",
       " Row(recommendationTitle='Rocky Marciano (1999)', baseTitle='Star Wars: Episode IV - A New Hope (1977)', recommendationId=141610, baseId=260, score=0.965195894241333),\n",
       " Row(recommendationTitle='Highlander (1986)', baseTitle='Star Wars: Episode IV - A New Hope (1977)', recommendationId=1275, baseId=260, score=0.9647436738014221),\n",
       " Row(recommendationTitle='Excalibur (1981)', baseTitle='Star Wars: Episode IV - A New Hope (1977)', recommendationId=2872, baseId=260, score=0.9644969701766968)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_movies = recommendations_to_titles(find_similar_movie(base_als_vectors, [260], 20))\n",
    "test_movies.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Предсказываем по 10 фильмов для каждого юзера из тестового датасета.\n",
    "\n",
    "Так как нет функционала для указания айдишников, которые не надо выдавать в результате (фильмы, которые уже юзер посмотрел выдавать бессмысленно), то предсказываю по 50, а затем фильтрую просмотренные и сокращаю до 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_movies(model, predict_df, watched_df, top=10):\n",
    "    def extractor(lst):\n",
    "        return [pr['movieId'] for pr in lst if pr['rating']>=3.]\n",
    "    \n",
    "    filter_udf = F.udf(extractor, ArrayType(IntegerType()))\n",
    "    \n",
    "    filtered_df = model\\\n",
    "        .recommendForUserSubset(predict_df.select('userId').distinct(), 50)\\\n",
    "        .withColumn('recommendations', filter_udf('recommendations'))\\\n",
    "        .cache()\n",
    "    \n",
    "    def limiter(lst):\n",
    "        return lst[:top]\n",
    "    \n",
    "    def none_replacer(lst):\n",
    "        if lst:\n",
    "            return lst\n",
    "        else:\n",
    "            return []\n",
    "    \n",
    "    limiter_udf = F.udf(limiter, ArrayType(IntegerType()))\n",
    "    replacer_udf = F.udf(none_replacer, ArrayType(IntegerType()))\n",
    "    \n",
    "    result = filtered_df\\\n",
    "        .join(watched_df, watched_df['user'] == filtered_df['userId'], 'left')\\\n",
    "        .withColumn('watched', replacer_udf('watched'))\\\n",
    "        .withColumn('recommendations', F.array_except(F.col('recommendations'), F.col('watched')))\\\n",
    "        .select('userId', 'recommendations')\\\n",
    "        .withColumn('recommendations', limiter_udf('recommendations'))\n",
    "    \n",
    "    return result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(userId=148, recommendations=[200930, 175391, 183947, 183845, 151989, 138224, 156414, 200872, 200936, 104119])]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_als_result = predict_movies(base_als_model, val_df, watched_df).cache()\n",
    "base_als_result.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MAP': 0.0,\n",
       " 'precision@1': 0.0,\n",
       " 'precision@10': 0.0,\n",
       " 'precision@5': 0.0,\n",
       " 'recall@1': 0.0,\n",
       " 'recall@10': 0.0,\n",
       " 'recall@5': 0.0}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Проверка, что не рекомендует фильмы из списка запрещенных. Все метрики должны быть по нулям.\n",
    "base_als_metrics = evaluate_movies(base_als_result, train_df)\n",
    "base_als_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MAP': 4.841065525501631e-07,\n",
       " 'precision@1': 6.37950392977443e-06,\n",
       " 'precision@10': 7.017454322751866e-06,\n",
       " 'precision@5': 6.3795039297744226e-06,\n",
       " 'recall@1': 2.809116657760643e-09,\n",
       " 'recall@10': 2.4525548413053207e-06,\n",
       " 'recall@5': 1.1797960390592066e-06}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_als_metrics = evaluate_movies(base_als_result, val_df)\n",
    "base_als_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_metrics = {}\n",
    "all_metrics['base ALS'] = base_als_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ваша формулировка\n",
    "\n",
    "На лекции было еще несколько ML формулировок задачи рекомендаций. Выберете одну из них и реализуйте метод."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Node2vec, который не работает"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GRAPH_PATH = os.path.join(DATA_PATH, 'rating_graph.edgelist')\n",
    "NODE2VEC_EMB_PATH = '/workspace/node2vec/result/ratings.emd'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "graph =nx.Graph()\n",
    "for line in train_df.collect():\n",
    "    graph.add_edge('u' + str(line['userId']), 'm' + str(line['movieId']), weight=line['rating'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from node2vec import Node2Vec\n",
    "\n",
    "node2vec = Node2Vec(graph, dimensions=10, walk_length=4, num_walks=1, workers=1)\n",
    "model = node2vec.fit(window=5, min_count=1, batch_words=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Основывалась на [этой статье](file:///home/katyakos/Downloads/3298689.3347041.pdf). Согласно табличке с результатами для рекомендации музыки, первые места занимают две нейронки, которые я не очень хотела обучать. А дальше идут простые формульные алгоритмы AR и SR из [этой статьи](https://arxiv.org/pdf/1803.09587.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Association Rules (AR)\n",
    "Session s --- chronologically ordered tuple of item click events s = (s1,s2,s3, . . . ,sm)\n",
    "\n",
    "S_p --- set of all past sessions\n",
    "\n",
    "User’s current session s with s_|s| being the last item interaction in s\n",
    "\n",
    "1(a,b) is 1 in case a and b refer to the same item and 0 otherwise\n",
    "\n",
    "score_{AR}(i,s) = (sum_{p in S_p} sum_{x=1..|p|} sum_{y=1..|p|} \\[ 1(s_|s|,p_x) * 1(i,p_y) \\]) / Z\n",
    "\n",
    "То есть берет последний фильм из текущей сессии и смотрит, сколько раз рассматриваемый для рекомендации фильм был с ним в одной из старых сессий. Считаем, что у одного юзера в трейне одна сплошная сессия, а прошлые сессии --- коллекция всех юзеров."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window   \n",
    "import pyspark.ml\n",
    "\n",
    "class AssociationRulesModel(pyspark.ml.Model):\n",
    "    \n",
    "    def __init__(self, scores_df, last_movie_df, watched_movies_df, most_popular_item,\n",
    "                 userCol):\n",
    "        self.scores_df = scores_df.cache()\n",
    "        self.last_movie_df = last_movie_df.cache()\n",
    "        self.watched_movies_df = watched_movies_df.cache()\n",
    "        self.most_popular_item = most_popular_item\n",
    "        self.userCol = userCol\n",
    "        \n",
    "           \n",
    "    def predict(self, df, top=10):\n",
    "        last_item_df = df.select(self.userCol).distinct()\\\n",
    "            .join(self.last_movie_df, self.userCol, 'left')\\\n",
    "            .fillna(self.most_popular_item)\\\n",
    "            .select(self.userCol, 'last_item')\n",
    "                \n",
    "        all_df = last_item_df\\\n",
    "            .join(self.scores_df, last_item_df['last_item'] == self.scores_df['key_item'])\n",
    "        \n",
    "        def none_replacer(lst):\n",
    "            if lst:\n",
    "                return lst\n",
    "            else:\n",
    "                return []\n",
    "            \n",
    "        def watch_filter(it, arr):\n",
    "            return it not in arr\n",
    "\n",
    "        replacer_udf = F.udf(none_replacer, ArrayType(IntegerType()))\n",
    "        filter_udf = F.udf(watch_filter, BooleanType())\n",
    "        window = Window.partitionBy(self.userCol).orderBy(F.col('score').desc())\n",
    "        \n",
    "        recommends_df = all_df\\\n",
    "            .join(self.watched_movies_df, self.userCol, 'left')\\\n",
    "            .withColumn('watched', replacer_udf('watched'))\\\n",
    "            .filter(filter_udf('val_item', 'watched'))\\\n",
    "            .withColumn('rank', F.rank().over(window))\\\n",
    "            .filter(F.col('rank') <= top)\\\n",
    "            .select(self.userCol, 'val_item', 'score')\\\n",
    "            .withColumn('recommendations', F.collect_list('val_item').over(window))\\\n",
    "            .select(self.userCol, 'recommendations')\\\n",
    "            .distinct()\n",
    "        \n",
    "        return recommends_df\n",
    "\n",
    "\n",
    "class AssociationRules(pyspark.ml.Estimator):\n",
    "    \n",
    "    def __init__(self, userCol, itemCol, orderCol):\n",
    "        self.userCol = userCol\n",
    "        self.itemCol = itemCol\n",
    "        self.orderCol = orderCol\n",
    "        \n",
    "    def fit(self, df) -> AssociationRulesModel:        \n",
    "        window_coocs = Window.partitionBy('key_item', 'val_item')\n",
    "        window_norm = Window.partitionBy('key_item')\n",
    "        \n",
    "        #Эта версия получения скоров долгая, но зато по памяти нормальная\n",
    "        '''schema = StructType([StructField('key_item',IntegerType(), True),\n",
    "                             StructField('val_item', IntegerType(), True),\n",
    "                             StructField('score', FloatType(), True)])\n",
    "        scores_df = sqlContext.createDataFrame(sc.emptyRDD(), schema)\n",
    "        item_list = [r['movieId'] for r in df.select(self.itemCol).distinct().collect()]\n",
    "        for keyId in item_list:\n",
    "            users_list = [r['userId'] for r in\n",
    "                          df.filter(F.col(self.itemCol) == keyId).select(self.userCol).distinct().collect()]\n",
    "            tmp = df\\\n",
    "                .filter(F.col(self.userCol).isin(users_list))\\\n",
    "                .select(self.itemCol)\\\n",
    "                .filter(F.col(self.itemCol) != keyId)\\\n",
    "                .withColumnRenamed(self.itemCol, 'val_item')\n",
    "            number = tmp.count()\n",
    "            tmp = tmp\\\n",
    "                .groupBy('val_item')\\\n",
    "                .agg(F.count('val_item').alias('score'))\\\n",
    "                .withColumn('score', F.col('score') / F.lit(number))\\\n",
    "                .withColumn('key_item', F.lit(keyId))\n",
    "            scores_df = scores_df.union(tmp)  '''        \n",
    "        \n",
    "        #Эта версия по памяти вообще не влезет, но быстрее работать будет\n",
    "        scores_df = df.withColumnRenamed(self.itemCol, 'key_item')\\\n",
    "            .join(df, self.userCol)\\\n",
    "            .withColumnRenamed(self.itemCol, 'val_item')\\\n",
    "            .filter(F.col('key_item') != F.col('val_item'))\\\n",
    "            .select('key_item', 'val_item')\\\n",
    "            .withColumn('coocs', F.count('val_item').over(window_coocs))\\\n",
    "            .withColumn('key_counts', F.count('val_item').over(window_norm))\\\n",
    "            .withColumn('score', F.col('coocs') / F.col('key_counts'))\\\n",
    "            .select('key_item', 'val_item', 'score')\n",
    "        \n",
    "        window_user = Window.partitionBy(self.userCol)\n",
    "        \n",
    "        last_movie_df = df\\\n",
    "            .withColumn('maxTime', F.max(self.orderCol).over(window_user))\\\n",
    "            .filter(F.col(self.orderCol) == F.col('maxTime'))\\\n",
    "            .withColumn('maxItem', F.max(self.itemCol).over(window_user))\\\n",
    "            .filter(F.col(self.itemCol) == F.col('maxItem'))\\\n",
    "            .select(self.userCol, self.itemCol)\\\n",
    "            .withColumnRenamed(self.itemCol, 'last_item')\n",
    "        \n",
    "        watched_movies_df = df\\\n",
    "            .groupby(self.userCol)\\\n",
    "            .agg(F.collect_set(self.itemCol).alias('watched'))\n",
    "        \n",
    "        most_popular_item = df\\\n",
    "            .groupBy(self.itemCol)\\\n",
    "            .agg(F.count(self.userCol).alias('count'))\\\n",
    "            .sort(F.col('count').desc())\\\n",
    "            .take(1)[0][self.itemCol]\n",
    "        \n",
    "        return AssociationRulesModel(scores_df, last_movie_df, watched_movies_df, most_popular_item,\n",
    "                                     self.userCol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_ar_model = AssociationRules('userId', 'movieId','timestamp')\n",
    "base_ar_model = base_ar_model.fit(train_df.filter(F.col('rating')>=3.).limit(2500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_ar_result = base_ar_model.predict(val_df).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(userId=496, recommendations=[1, 1, 260, 260, 356, 356, 440, 440, 910, 910, 1196, 1196, 1210, 1210, 1234, 1234, 1270, 1270, 1304, 1304, 1485, 1485, 2067, 2067, 5060, 5060]),\n",
       " Row(userId=833, recommendations=[2571, 2571, 2571, 2571, 2571, 2571, 2571, 2571, 48516, 48516, 48516, 48516, 48516, 48516, 48516, 48516])]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_ar_result.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_ar_result.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df.select('userId').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Проверка, что не рекомендует фильмы из списка запрещенных. Все метрики должны быть по нулям.\n",
    "base_ar_metrics = evaluate_movies(base_ar_result, train_df)\n",
    "base_ar_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_ar_metrics = evaluate_movies(base_ar_result, val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_metrics = {}\n",
    "all_metrics['base AR'] = base_ar_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Results\n",
    "\n",
    "Сравните реализованные методы с помощью выбранных метрик. Не забывайте про оптимизацию гиперпараметров."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################\n",
    "######### YOUR CODE HERE #############\n",
    "######################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
