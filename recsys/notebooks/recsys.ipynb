{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Movie-recommendation\" data-toc-modified-id=\"Movie-recommendation-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Movie recommendation</a></span><ul class=\"toc-item\"><li><span><a href=\"#Dataset\" data-toc-modified-id=\"Dataset-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Dataset</a></span></li><li><span><a href=\"#Evaluation-Protocol\" data-toc-modified-id=\"Evaluation-Protocol-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Evaluation Protocol</a></span></li><li><span><a href=\"#Models\" data-toc-modified-id=\"Models-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Models</a></span><ul class=\"toc-item\"><li><span><a href=\"#ALS\" data-toc-modified-id=\"ALS-1.3.1\"><span class=\"toc-item-num\">1.3.1&nbsp;&nbsp;</span><a href=\"https://spark.apache.org/docs/latest/ml-collaborative-filtering.html#explicit-vs-implicit-feedback\" target=\"_blank\">ALS</a></a></span></li><li><span><a href=\"#Ваша-формулировка\" data-toc-modified-id=\"Ваша-формулировка-1.3.2\"><span class=\"toc-item-num\">1.3.2&nbsp;&nbsp;</span>Ваша формулировка</a></span></li></ul></li><li><span><a href=\"#Evaluation-Results\" data-toc-modified-id=\"Evaluation-Results-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Evaluation Results</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movie recommendation\n",
    "\n",
    "Ваша задача - рекомендация фильмов для пользователей\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format ='retina'\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import pyspark\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master('local[*]') \\\n",
    "    .appName(\"spark_sql_examples\") \\\n",
    "    .config(\"spark.executor.memory\", \"25g\") \\\n",
    "    .config(\"spark.driver.memory\", \"25g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset \n",
    "\n",
    "`MovieLens-25M`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "genome-scores.csv  links.csv   ratings.csv  tags.csv\r\n",
      "genome-tags.csv    movies.csv  README.txt\r\n"
     ]
    }
   ],
   "source": [
    "!ls /workspace/ml-25m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '/workspace/ml-25m'\n",
    "\n",
    "RATINGS_PATH = os.path.join(DATA_PATH, 'ratings.csv')\n",
    "MOVIES_PATH = os.path.join(DATA_PATH, 'movies.csv')\n",
    "TAGS_PATH = os.path.join(DATA_PATH, 'tags.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "\n",
    "ratings_df = sqlContext.read.format(\"com.databricks.spark.csv\") \\\n",
    "    .option(\"delimiter\", \",\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load('file:///' + RATINGS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "old = ratings_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_df = old.limit(10000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df = sqlContext.read.format(\"com.databricks.spark.csv\") \\\n",
    "    .option(\"delimiter\", \",\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load('file:///' + MOVIES_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(userId=1, movieId=296, rating=5.0, timestamp=1147880044)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_df.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(movieId=1, title='Toy Story (1995)', genres='Adventure|Animation|Children|Comedy|Fantasy')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_df.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Protocol\n",
    "\n",
    "Так как мы хотим оценивать качество разных алгоритмов рекомендаций, то в первую очередь нам нужно определить\n",
    "* Как разбить данные на `Train`/`Validation`/`Test`;\n",
    "* Какие метрики использовать для оценки качества."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metrics import get_ate\n",
    "from processing import split_by_col, partitioned_split_by_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Protocol 1 --- split by timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = split_by_col(ratings_df, 'timestamp', [0.85, 0.15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of users in test but not in train: 17993\n"
     ]
    }
   ],
   "source": [
    "new_test_users_num = test_df.select('userId').distinct().subtract(train_df.select('userId').distinct()).count()\n",
    "print(\"Number of users in test but not in train:\", new_test_users_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of users in train: 144548\n",
      "Number of users in test: 24525\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of users in train:\", train_df.select('userId').distinct().count())\n",
    "print(\"Number of users in test:\", test_df.select('userId').distinct().count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number in train: 21250080\n",
      "Number in test: 3750014\n"
     ]
    }
   ],
   "source": [
    "print(\"Number in train:\", train_df.count())\n",
    "print(\"Number in test:\", test_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_users = test_df.select('userId').distinct().intersect(train_df.select('userId').distinct()).collect()\n",
    "good_users = [user['userId'] for user in good_users]\n",
    "test_df = test_df\\\n",
    "    .filter(F.col('userId').isin(good_users))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Protocol 2 --- split each user by timestamp and unite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = partitioned_split_by_cols(ratings_df, 'userId', 'timestamp', [0.85, 0.15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of users in test but not in train: 0\n"
     ]
    }
   ],
   "source": [
    "new_test_users_num = test_df.select('userId').distinct().subtract(train_df.select('userId').distinct()).count()\n",
    "print(\"Number of users in test but not in train:\", new_test_users_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of users in train: 162541\n",
      "Number of users in test: 159964\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of users in train:\", train_df.select('userId').distinct().count())\n",
    "print(\"Number of users in test:\", test_df.select('userId').distinct().count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number in train: 21264618\n",
      "Number in test: 3598611\n"
     ]
    }
   ],
   "source": [
    "print(\"Number in train:\", train_df.count())\n",
    "print(\"Number in test:\", test_df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Protocols consensus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В первом способе получилось слишком много новых пользователей в тесте и валидейте, так что использовать в так виде или оставлять только \"общих\" пользователей кажется бессмысленным. Во втором способе этой проблемы нет. Теоретическая проблема: заглядывание в будущее. Но мы по сути заглядываем в будущее для оценок фильмов. А фильмы это фиксированный объект, со временем не меняется. Так что кажется, что ничего страшного в этом нет.\n",
    "\n",
    "Далее используется разделение датасета вторым способом.\n",
    "\n",
    "Считаем, что стоит рекомендовать посмотреть, если рейтинг поставленный >=3 (в оценке)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_df.filter(F.col('rating') >= 3.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Protocols consensus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При рекомендации фильмов обычно число k в top-k не очень большое, так что, кажется, порядок не так важен. Интереса ради используется одна метрика порядка (MAP). Но основными метриками будут Precision@k, Recall@k. HR@k не используется, так как не очень интересно, сколько раз удалось пользователю посоветовать ХОТЬ ОДИН хороший фильм. Хочется скорее узнать среднее число удачных советов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "\n",
    "def MAP(recommended_df, true_df):\n",
    "    users_num = true_df.select('userId').distinct().count()\n",
    "    window = Window.partitionBy('userId').orderBy(F.col('rating').desc()).rowsBetween(-sys.maxsize, 0)\n",
    "    recommended_df = recommended_df\\\n",
    "        .withColumn('one', F.lit(1))\\\n",
    "        .withColumn('rec_weight', F.lit(1.) / F.sum(F.col('one')).over(window))\\\n",
    "        .drop('one')\\\n",
    "        .withColumnRenamed('userId', 'user')\\\n",
    "        .withColumnRenamed('movieId', 'movie')\n",
    "    \n",
    "    window = Window.partitionBy('userId')\n",
    "    true_df = true_df.withColumn('rel_count', F.count(F.col('movieId')).over(window))\n",
    "    result = true_df\\\n",
    "        .join(recommended_df, [true_df['userId'] == recommended_df['user'], true_df['movieId'] == recommended_df['movie']])\\\n",
    "        .select('userId', 'movieId', 'rel_count', 'rec_weight')\\\n",
    "        .withColumn('total_weight', F.sum(F.col('rec_weight')).over(window))\\\n",
    "        .select('userId', 'rel_count', 'total_weight')\\\n",
    "        .distinct()\\\n",
    "        .withColumn('total_weight', F.col('total_weight') / F.col('rel_count'))\\\n",
    "        .select('total_weight')\\\n",
    "        .groupBy().agg(F.sum('total_weight').alias('sum')).take(1)[0]['sum']\n",
    "    return result / users_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "\n",
    "def precision_at_k(recommended_df, true_df):\n",
    "    users_num = true_df.select('userId').distinct().count()\n",
    "    window = Window.partitionBy('userId')\n",
    "    recommended_df = recommended_df.withColumn('rec_count', F.count(F.col('movieId')).over(window))\\\n",
    "        .withColumnRenamed('userId', 'user')\\\n",
    "        .withColumnRenamed('movieId', 'movie')\n",
    "    result = true_df\\\n",
    "        .join(recommended_df, [true_df['userId'] == recommended_df['user'], true_df['movieId'] == recommended_df['movie']])\\\n",
    "        .select('userId', 'rec_count')\\\n",
    "        .withColumn('total', F.count(F.col('userId')).over(window) / F.col('rec_count'))\\\n",
    "        .distinct()\\\n",
    "        .select('total')\\\n",
    "        .groupBy().agg(F.sum('total').alias('sum')).take(1)[0]['sum']\n",
    "    return result / users_num\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_at_k(recommended_df, true_df):\n",
    "    users_num = true_df.select('userId').distinct().count()\n",
    "    window = Window.partitionBy('userId')\n",
    "    true_df = true_df.withColumn('rel_count', F.count(F.col('movieId')).over(window))\\\n",
    "        .withColumnRenamed('userId', 'user')\\\n",
    "        .withColumnRenamed('movieId', 'movie')\n",
    "\n",
    "    result = recommended_df\\\n",
    "        .join(true_df, [true_df['user'] == recommended_df['userId'], true_df['movie'] == recommended_df['movieId']])\\\n",
    "        .select('userId', 'rel_count')\\\n",
    "        .withColumn('total', F.count(F.col('userId')).over(window) / F.col('rel_count'))\\\n",
    "        .distinct()\\\n",
    "        .select('total')\\\n",
    "        .groupBy().agg(F.sum('total').alias('sum')).take(1)[0]['sum']\n",
    "    return result / users_num\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n",
    "\n",
    "Теперь мы можем перейти к формулировке задачи в терминах машинного обучения.\n",
    "\n",
    "Одна из формулировок, к которой мы сведем нашу задачу - **Matrix Completetion**. Данную задачу будем решать с помощью `ALS`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [ALS](https://spark.apache.org/docs/latest/ml-collaborative-filtering.html#explicit-vs-implicit-feedback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.recommendation import ALS\n",
    "base_als = ALS(maxIter=10, rank=10, regParam=.1, userCol='userId', itemCol='movieId', ratingCol='rating', \n",
    "               coldStartStrategy='drop', implicitPrefs=False)\n",
    "base_als_model = base_als.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=10, features=[-0.6285361051559448, 0.2037687748670578, 0.6077628135681152, -0.52348393201828, 0.18761347234249115, -0.054997917264699936, 0.058772288262844086, -0.9275143146514893, 0.32570794224739075, 0.507000744342804]),\n",
       " Row(id=20, features=[-0.44506630301475525, 0.4190051555633545, 0.45511385798454285, -0.5662184357643127, 0.4383411109447479, 0.2585639953613281, 0.12199082225561142, -0.8230255246162415, -0.13716013729572296, 0.20074614882469177])]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_als_vectors = base_als_model.itemFactors\n",
    "base_als_vectors.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Покажите для выбранных вами фильмов топ-20 наиболее похожих фильмов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommendations_to_titles(data):\n",
    "    tmp = movies_df.select('movieId', 'title')\\\n",
    "        .join(data, movies_df['movieId'] == data['baseId'])\\\n",
    "        .drop('movieId')\\\n",
    "        .withColumnRenamed('title', 'baseTitle')\n",
    "    return movies_df.select('movieId', 'title')\\\n",
    "        .join(tmp, movies_df['movieId'] == tmp['recommendationId'])\\\n",
    "        .drop('movieId')\\\n",
    "        .withColumnRenamed('title', 'recommendationTitle')\\\n",
    "        .sort(F.col('score').asc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf, array\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "\n",
    "def cosine_sim(array):\n",
    "        return float(cosine(array[0], array[1]))\n",
    "\n",
    "\n",
    "def find_similar_movie(vectors_df, movieIds, topK):\n",
    "    base_df = vectors_df\\\n",
    "        .filter(F.col('id').isin(movieIds))\\\n",
    "        .withColumnRenamed('id', 'baseId')\\\n",
    "        .withColumnRenamed('features', 'baseFeatures')\n",
    "    \n",
    "    dot_udf = udf(cosine_sim, FloatType())\n",
    "\n",
    "    \n",
    "    result = vectors_df\\\n",
    "        .join(base_df, base_df['baseId'] != vectors_df['id'])\\\n",
    "        .withColumn('score', dot_udf(array('features', 'baseFeatures')))\\\n",
    "        .sort(F.col('score').asc())\\\n",
    "        .limit(topK)\\\n",
    "        .drop('features', 'baseFeatures')\\\n",
    "        .withColumnRenamed('id', 'recommendationId')\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(recommendationTitle='Star Wars: Episode V - The Empire Strikes Back (1980)', baseTitle='Star Wars: Episode IV - A New Hope (1977)', recommendationId=1196, baseId=260, score=0.0037523782812058926),\n",
       " Row(recommendationTitle='Star Wars: Episode VI - Return of the Jedi (1983)', baseTitle='Star Wars: Episode IV - A New Hope (1977)', recommendationId=1210, baseId=260, score=0.014959782361984253),\n",
       " Row(recommendationTitle='Raiders of the Lost Ark (Indiana Jones and the Raiders of the Lost Ark) (1981)', baseTitle='Star Wars: Episode IV - A New Hope (1977)', recommendationId=1198, baseId=260, score=0.018733611330389977),\n",
       " Row(recommendationTitle='Indiana Jones and the Last Crusade (1989)', baseTitle='Star Wars: Episode IV - A New Hope (1977)', recommendationId=1291, baseId=260, score=0.02520429901778698),\n",
       " Row(recommendationTitle='Star Trek II: The Wrath of Khan (1982)', baseTitle='Star Wars: Episode IV - A New Hope (1977)', recommendationId=1374, baseId=260, score=0.027708830311894417),\n",
       " Row(recommendationTitle='Paparazzi (1998)', baseTitle='Star Wars: Episode IV - A New Hope (1977)', recommendationId=138290, baseId=260, score=0.03654107823967934),\n",
       " Row(recommendationTitle='San Antonio (1945)', baseTitle='Star Wars: Episode IV - A New Hope (1977)', recommendationId=8976, baseId=260, score=0.04249918833374977),\n",
       " Row(recommendationTitle='Sillunu Oru Kaadhal (2006)', baseTitle='Star Wars: Episode IV - A New Hope (1977)', recommendationId=158294, baseId=260, score=0.044080447405576706),\n",
       " Row(recommendationTitle='How to Murder Your Wife (1965)', baseTitle='Star Wars: Episode IV - A New Hope (1977)', recommendationId=5800, baseId=260, score=0.04427972435951233),\n",
       " Row(recommendationTitle='Raiders of the Lost Ark: The Adaptation (1989)', baseTitle='Star Wars: Episode IV - A New Hope (1977)', recommendationId=69524, baseId=260, score=0.04440143704414368)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_movies = recommendations_to_titles(find_similar_movie(base_als_vectors, [260], 20))\n",
    "test_movies.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Предсказываем по 10 фильмов для каждого юзера из тестового датасета.\n",
    "\n",
    "Так как нет функционала для указания айдишников, которые не надо выдавать в результате (фильмы, которые уже в трейне юзер посмотрел выдавать бессмысленно), то предсказываю по 30, а затем фильтрую и сокращаю."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(userId=9427, movieId=151989, rating=6.758526802062988),\n",
       " Row(userId=9427, movieId=101862, rating=6.072198867797852)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_als_result = base_als_model.recommendForUserSubset(test_df.select('userId').distinct(), 30)\\\n",
    "    .withColumn('rec', F.explode(F.col('recommendations'))).drop('recommendations')\\\n",
    "    .withColumn('movieId', F.col('rec')['movieId'])\\\n",
    "    .withColumn('rating', F.col('rec')['rating']).drop('rec')\n",
    "base_als_result.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(userId=9427, movieId=151989, rating=6.758526802062988),\n",
       " Row(userId=9427, movieId=101862, rating=6.072198867797852)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "window = Window.partitionBy('userId').orderBy(F.col('rating').desc()).rowsBetween(-sys.maxsize, 0)\n",
    "\n",
    "tmp = base_als_result.select('userId', 'movieId')\\\n",
    "    .subtract(train_df.select('userId', 'movieId'))\\\n",
    "    .withColumnRenamed('userId', 'user')\\\n",
    "    .withColumnRenamed('movieId', 'movie')\n",
    "tmp = base_als_result\\\n",
    "    .join(tmp, [tmp['user'] == base_als_result['userId'], tmp['movie'] == base_als_result['movieId']])\\\n",
    "    .withColumn('one', F.lit(1))\\\n",
    "    .withColumn('number', F.sum(F.col('one')).over(window))\\\n",
    "    .filter(F.col('number') <= 10)\\\n",
    "    .select('userId', 'movieId', 'rating')\n",
    "tmp.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_als_result = tmp.filter(F.col('rating') >= 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0029593094944512957"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_at_k(base_als_result, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_metrics = {}\n",
    "base_als_metrics = {\n",
    "    'precision@k': precision_at_k(base_als_result, test_df),\n",
    "    'recall@k': recall_at_k(base_als_result, test_df),\n",
    "    'MAP': MAP(base_als_result, test_df)\n",
    "}\n",
    "\n",
    "all_metrics['base ALS'] = base_als_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'base ALS': {'MAP': 7.877653718243697e-05,\n",
       "  'precision@k': 0.0029593094944512957,\n",
       "  'recall@k': 0.0003514820684581914}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ваша формулировка\n",
    "\n",
    "На лекции было еще несколько ML формулировок задачи рекомендаций. Выберете одну из них и реализуйте метод."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "GRAPH_PATH = os.path.join(DATA_PATH, 'rating_graph.edgelist')\n",
    "NODE2VEC_EMB_PATH = '/workspace/node2vec/result/ratings.emd'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "graph =nx.Graph()\n",
    "for line in train_df.collect():\n",
    "    graph.add_edge('u' + str(line['userId']), 'm' + str(line['movieId']), weight=line['rating'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing transition probabilities:   0%|          | 10/183267 [02:42<1216:19:53, 23.89s/it]"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "from node2vec import Node2Vec\n",
    "\n",
    "node2vec = Node2Vec(graph, dimensions=10, walk_length=4, num_walks=1, workers=16)\n",
    "model = node2vec.fit(window=5, min_count=1, batch_words=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Results\n",
    "\n",
    "Сравните реализованные методы с помощью выбранных метрик. Не забывайте про оптимизацию гиперпараметров."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################\n",
    "######### YOUR CODE HERE #############\n",
    "######################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
